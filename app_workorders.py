# app_workorders.py ‚Äî Parquet fast‚Äëpath + Service Procedure Filter (local)
# ---------------------------------------------------------------------------------
# SPF Work Orders (reads local Workorders.xlsx by default; Parquet cache)
# Pages: Asset History ‚Ä¢ Work Orders ‚Ä¢ Service Report ‚Ä¢ Service History ‚Ä¢ Service Procedure Filter
# Privacy-safe by Location; Dates normalized; ‚ÄúData last updated‚Äù (ET)
# Requires: pandas, pyarrow, xlsxwriter, streamlit-authenticator, openpyxl, python-docx
# ---------------------------------------------------------------------------------

from __future__ import annotations
import io, re, os
from pathlib import Path
from collections.abc import Mapping
from datetime import datetime, timedelta, timezone

import pandas as pd
import streamlit as st
import yaml

APP_VERSION = "2025.10.15-spf1"

# ---------- small CSS: hide Streamlit chrome / shrink controls ----------
st.set_page_config(page_title="SPF Work Orders", page_icon="üß∞", layout="wide")
st.markdown(
    """
    <style>
      /* Hide viewer badge / deploy button / footer */
      .stDeployButton, footer, header {visibility: hidden;}
      [data-testid="stDecoration"] {display:none;}
      .viewerBadge_container__E0v7 {display:none !important;}
      /* Tighter selectboxes */
      div[data-baseweb="select"] > div {min-height: 34px;}
      label[for] {margin-bottom: 2px;}
    </style>
    """,
    unsafe_allow_html=True,
)

# ---------- deps ----------
try:
    import streamlit_authenticator as stauth
except Exception:
    st.error("streamlit-authenticator not installed. Add it to requirements.txt")
    st.stop()

# NOTE: python-docx is imported lazily inside to_docx_bytes() / proc_to_docx_bytes() for faster cold start

# ---------- constants ----------
LOCAL_XLSX_DEFAULT = "Workorders.xlsx"

SHEET_WORKORDERS         = "Workorders"                  # history sheet
SHEET_ASSET_MASTER       = "Asset_Master"
SHEET_WO_MASTER          = "Workorders_Master"           # listing sheet with flags

# Service History sheet names (your current is Workorders_Master_Services)
SHEET_WO_SERVICE_CANDS   = [
    "Workorders_Master_Services",
    "Workorders_Master_service",
    "Workorders_Master_Service",
    "Workorders Service",
    "Service History"
]

# Service Report sheet candidate names
SHEET_SERVICE_CANDIDATES = ["Service Report", "Service_Report", "ServiceReport"]

# Optional Users sheet
SHEET_USERS_CANDIDATES   = ["Users", "Users]", "USERS", "users"]

REQUIRED_WO_COLS = [
    "WORKORDER","TITLE","STATUS","PO","P/N","QUANTITY RECEIVED",
    "Vendors","COMPLETED ON","ASSET","Location",
]
OPTIONAL_SORT_COL = "Sort"
ASSET_MASTER_COLS = ["Location","ASSET"]

MASTER_REQUIRED = [
    "ID","Title","Description","Asset","Status","Created on","Planned Start Date",
    "Due date","Started on","Completed on","Assigned to","Teams Assigned to",
    "Completed by","Location","IsOpen","IsOverdue","IsScheduled","IsCompleted","IsOld"
]

# Canon for Service Report
SERVICE_REPORT_CANON = {
    "WO_ID":{"workorder","wo","work order","work order id","id","wo id"},
    "Title":{"title","name"},
    "Service":{"service","procedure","procedure name","task","step","line item"},
    "Asset":{"asset","asset name"},
    "Location":{"location","ns location","location2"},
    "Date":{"date","completed on","performed on","service date","closed on"},
    "User":{"user","technician","completed by","performed by","assigned to"},
    "Notes":{"notes","description","comment","comments","details"},
    "Status":{"status"},
    "Schedule":{"schedule","interval","frequency","meter interval","planned interval","cycle"},
    "Remaining":{"remaining","remaining value","units remaining","miles remaining","hours remaining","reading remaining","remaining units"},
    "Percent Remaining":{"percent remaining","% remaining","remaining %","remaining pct","pct remaining"},
    "Meter Type":{"meter type","type","uom","unit","units"},
    "Due Date":{"due date","next due","target date","next service date"},
    # If your Service Report ever includes MReading, this will map it
    "MReading":{"mreading","meter reading","reading at service","reading"},
}

# Canon for Service History (Workorders_Master_Services)
SERVICE_HISTORY_CANON = {
    "WO_ID":{"id","wo","workorder","work order","workorder id"},
    "Title":{"title"},
    "Service":{"service type","service","procedure name","procedure","task"},
    "Asset":{"asset","asset name"},
    "Location2":{"location2","location","ns location"},
    "Date":{"completed on","performed on","date","service date"},
    "User":{"completed by","technician","assigned to","performed by","user"},
    "Notes":{"notes","description","comment","comments","details"},
    "Status":{"status"},
    "MReading":{"mreading","meter reading","reading at service","reading"},
    "MHours":{"mhours","hours at service","hours"},
}

# ---------- NEW: Service Procedure Filter (local) ----------
SHEET_PROC     = "Service Procedures"
SHEET_CTRL     = "Controls"
SHEET_INV      = "Parts_Master"
SHEET_XREF     = "Filter_List"

COL_ASSET    = "Asset"
COL_SERIAL   = "Serial"
COL_SERVICE  = "Service"
COL_TASKNO   = "Task #"
COL_TASK     = "Task"
COL_LOC_LIST = "Locations"

INV_COL_PN    = "Part Numbers"
INV_COL_QTY   = "Quantity in Stock"
INV_COL_LOC   = "Location"
INV_COL_AREA  = "Area"
INV_COL_LOC2  = "Location2"

XREF_COL_CAT  = "Cat"
XREF_COL_DON  = "Donaldson P/N"

INV_NAME_CANDIDATES = [
    "Name", "Description", "Item", "Part Name", "Description 1",
    "Item Description", "Product Name"
]

PART_PREFIXES = ("Part Number_", "Part Type_", "Description_", "Qty_")

# ---------- helpers ----------
def to_plain(obj):
    if isinstance(obj, Mapping):
        return {k: to_plain(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return [to_plain(x) for x in obj]
    return obj

def load_config() -> dict:
    if "app_config" in st.secrets:
        return to_plain(st.secrets["app_config"])
    if "app_config_yaml" in st.secrets:
        try:
            return yaml.safe_load(st.secrets["app_config_yaml"]) or {}
        except Exception as e:
            st.error(f"Invalid YAML in app_config_yaml secret: {e}")
            return {}
    here = Path(__file__).resolve().parent
    cfg_file = here / "app_config.yaml"
    if cfg_file.exists():
        try:
            return yaml.safe_load(cfg_file.read_text(encoding="utf-8")) or {}
        except Exception as e:
            st.error(f"Invalid YAML in app_config.yaml: {e}")
            return {}
    return {}

def _norm_date_any(s: str) -> str:
    s = (str(s) if s is not None else "").strip()
    if not s:
        return ""
    for fmt in ("%Y-%m-%d","%m/%d/%Y","%m/%d/%y","%d-%b-%Y","%Y-%m-%d %H:%M:%S"):
        try:
            return datetime.strptime(s, fmt).strftime("%Y-%m-%d")
        except Exception:
            pass
    try:
        dt = pd.to_datetime(s, errors="coerce")
        return "" if pd.isna(dt) else dt.strftime("%Y-%m-%d")
    except Exception:
        return s

def _norm_key(x: str) -> str:
    s = re.sub(r"[^0-9a-z]+", " ", str(x).lower())
    return re.sub(r"\s+", " ", s).strip()

def _canonize_headers(df: pd.DataFrame, canon: dict[str, set[str]]) -> pd.DataFrame:
    low_to_orig = {str(c).strip().lower(): str(c) for c in df.columns}
    mapping = {}
    for key, aliases in canon.items():
        key_l = key.lower()
        if key_l in low_to_orig:
            mapping[low_to_orig[key_l]] = key
            continue
        for low, orig in low_to_orig.items():
            low2 = re.sub(r"\s+", " ", low)
            if low in aliases or low2 in aliases:
                mapping[orig] = key
                break
    return df.rename(columns=mapping)

def to_xlsx_bytes(df: pd.DataFrame, sheet: str) -> bytes:
    import xlsxwriter  # ensure dependency exists; used only on download
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as w:
        df.to_excel(w, index=False, sheet_name=sheet)
        ws = w.sheets[sheet]
        ws.autofilter(0, 0, max(0, len(df)), max(0, len(df.columns)-1))
        for i, col in enumerate(df.columns):
            width = 12 if df.empty else min(60, max(10, int(df[col].astype(str).str.len().quantile(0.9)) + 2))
            ws.set_column(i, i, width)
    return buf.getvalue()

def to_docx_bytes(df: pd.DataFrame, title: str) -> bytes:
    from docx import Document
    from docx.shared import Pt
    doc = Document()
    doc.styles["Normal"].font.name = "Calibri"
    doc.styles["Normal"].font.size = Pt(10)
    doc.add_heading(title, level=1)
    rows, cols = len(df)+1, len(df.columns)
    tbl = doc.add_table(rows=rows, cols=cols); tbl.style = "Table Grid"
    for j, c in enumerate(df.columns): tbl.cell(0, j).text = str(c)
    for i, (_, r) in enumerate(df.iterrows(), start=1):
        for j, c in enumerate(df.columns):
            v = "" if pd.isna(r[c]) else str(r[c]); tbl.cell(i, j).text = v
    out = io.BytesIO(); doc.save(out); return out.getvalue()

def coerce_bool(s: pd.Series) -> pd.Series:
    if s.dtype == bool: return s
    m = s.astype(str).str.strip().str.lower()
    true_vals  = {"true","yes","y","1","t"}
    false_vals = {"false","no","n","0","f","", "nan", "none"}
    out = m.map(lambda x: True if x in true_vals else (False if x in false_vals else False))
    return out.astype(bool)

# ---------- fast I/O / Parquet layer ----------
PARQUET_DIR = Path("faststore")
PARQUET_DIR.mkdir(exist_ok=True)

def _xlsx_path_from_cfg(cfg: dict) -> Path:
    return Path((cfg.get("settings", {}) or {}).get("xlsx_path") or LOCAL_XLSX_DEFAULT)

def _proc_xlsx_path_from_cfg(cfg: dict) -> Path:
    # Optional separate workbook for procedures; falls back to main
    return Path((cfg.get("settings", {}) or {}).get("proc_xlsx_path") or (cfg.get("settings", {}) or {}).get("xlsx_path") or LOCAL_XLSX_DEFAULT)

def _mtime(path: Path) -> float:
    try:
        return path.stat().st_mtime
    except Exception:
        return 0.0

@st.cache_data(show_spinner=False)
def get_local_xlsx_bytes_cached(xlsx_path_str: str, xlsx_mtime: float) -> bytes:
    # cache keyed by path + mtime so reruns don't re-read file from disk
    return Path(xlsx_path_str).read_bytes()

def _pq_path(sheet: str) -> Path:
    safe = re.sub(r"[^0-9A-Za-z_.-]+", "_", sheet.strip())
    return PARQUET_DIR / f"{safe}.parquet"

def _write_parquet(df: pd.DataFrame, sheet: str) -> None:
    df.to_parquet(_pq_path(sheet), index=False)

def _parquet_fresh(sheet: str, xlsx_path: Path) -> bool:
    pq = _pq_path(sheet)
    return pq.exists() and _mtime(pq) >= _mtime(xlsx_path)

def _read_parquet_columns(sheet: str, columns: list[str] | None = None) -> pd.DataFrame:
    return pd.read_parquet(_pq_path(sheet), columns=columns)

def _rebuild_parquet_from_excel(xlsx_bytes: bytes, sheets_to_pull: list[str]) -> dict[str, pd.DataFrame]:
    # Parse multiple sheets with one Excel decode, then write parquet
    with pd.ExcelFile(io.BytesIO(xlsx_bytes), engine="openpyxl") as xf:
        out = {}
        for sh in sheets_to_pull:
            try:
                df = xf.parse(sh, dtype=str, keep_default_na=False)
                df.columns = [str(c).strip() for c in df.columns]
                _write_parquet(df, sh)
                out[sh] = df
            except Exception:
                pass
    return out

def parquet_button_and_refresh(xlsx_path: Path, xlsx_bytes: bytes) -> None:
    with st.sidebar.expander("‚ö° Data cache"):
        if st.button("Rebuild Parquet cache now"):
            _rebuild_parquet_from_excel(xlsx_bytes, [
                SHEET_WORKORDERS, SHEET_ASSET_MASTER, SHEET_WO_MASTER,
                *SHEET_WO_SERVICE_CANDS, *SHEET_SERVICE_CANDIDATES, *SHEET_USERS_CANDIDATES,
                SHEET_PROC, SHEET_CTRL, SHEET_INV, SHEET_XREF
            ])
            st.success("Parquet cache rebuilt.")
            st.cache_data.clear()
            st.rerun()

# ---------- data access ----------
def get_local_xlsx_bytes(cfg: dict) -> bytes:
    p = _xlsx_path_from_cfg(cfg)
    if not p.exists():
        raise FileNotFoundError(f"Local Excel not found: {p.resolve()}")
    return get_local_xlsx_bytes_cached(str(p), _mtime(p))

def get_local_proc_xlsx_bytes(cfg: dict) -> tuple[bytes, Path]:
    p = _proc_xlsx_path_from_cfg(cfg)
    if not p.exists():
        # fallback to main workbook
        p = _xlsx_path_from_cfg(cfg)
    return get_local_xlsx_bytes_cached(str(p), _mtime(p)), p

def get_data_last_updated_local(cfg: dict) -> str | None:
    xl = (cfg.get("settings", {}) or {}).get("xlsx_path") or LOCAL_XLSX_DEFAULT
    try:
        from zoneinfo import ZoneInfo
        ts = datetime.fromtimestamp(os.path.getmtime(xl), tz=timezone.utc).astimezone(ZoneInfo("America/New_York"))
        return ts.strftime("Data last updated: %Y-%m-%d %H:%M ET")
    except Exception:
        return None

# ---------- loaders (Parquet fast-path) ----------
@st.cache_data(show_spinner=False)
def load_workorders_df(xlsx_mtime: float, xlsx_path_str: str) -> pd.DataFrame:
    need = REQUIRED_WO_COLS + ([OPTIONAL_SORT_COL] if OPTIONAL_SORT_COL else [])
    sheet = SHEET_WORKORDERS
    xlsx_path = Path(xlsx_path_str)

    if _parquet_fresh(sheet, xlsx_path):
        df = _read_parquet_columns(sheet, columns=[c for c in need if c])
    else:
        _rebuild_parquet_from_excel(get_local_xlsx_bytes_cached(xlsx_path_str, xlsx_mtime), [sheet])
        df = _read_parquet_columns(sheet, columns=[c for c in need if c])

    df.columns = [str(c).strip() for c in df.columns]
    missing = [c for c in REQUIRED_WO_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"Sheet '{sheet}' missing columns: {missing}\nFound: {list(df.columns)}")

    df = df[[c for c in need if c in df.columns]].copy()
    if "COMPLETED ON" in df.columns:
        df["COMPLETED ON"] = df["COMPLETED ON"].map(_norm_date_any)
    for c in df.columns:
        if df[c].dtype == object:
            df[c] = df[c].astype("string").str.strip()
    for catcol in ("Location","ASSET"):
        if catcol in df.columns:
            df[catcol] = df[catcol].astype("category")
    return df

@st.cache_data(show_spinner=False)
def load_asset_master_df(xlsx_mtime: float, xlsx_path_str: str) -> pd.DataFrame:
    sheet = SHEET_ASSET_MASTER
    xlsx_path = Path(xlsx_path_str)
    if _parquet_fresh(sheet, xlsx_path):
        df = _read_parquet_columns(sheet, columns=ASSET_MASTER_COLS)
    else:
        _rebuild_parquet_from_excel(get_local_xlsx_bytes_cached(xlsx_path_str, xlsx_mtime), [sheet])
        df = _read_parquet_columns(sheet, columns=ASSET_MASTER_COLS)

    df.columns = [str(c).strip() for c in df.columns]
    for c in ASSET_MASTER_COLS:
        if c not in df.columns:
            raise ValueError(f"Sheet '{sheet}' missing '{c}'")
        df[c] = df[c].astype("string").str.strip()
    df = df[(df["Location"] != "") & (df["ASSET"] != "")]
    df["Location"] = df["Location"].astype("category")
    df["ASSET"] = df["ASSET"].astype("category")
    return df[ASSET_MASTER_COLS].copy()

@st.cache_data(show_spinner=False)
def load_wo_master_df(xlsx_mtime: float, xlsx_path_str: str) -> pd.DataFrame:
    sheet = SHEET_WO_MASTER
    xlsx_path = Path(xlsx_path_str)
    if _parquet_fresh(sheet, xlsx_path):
        df = _read_parquet_columns(sheet, columns=None)
    else:
        _rebuild_parquet_from_excel(get_local_xlsx_bytes_cached(xlsx_path_str, xlsx_mtime), [sheet])
        df = _read_parquet_columns(sheet, columns=None)

    df.columns = [str(c).strip() for c in df.columns]
    have = [c for c in MASTER_REQUIRED if c in df.columns]
    if have:
        df = df[have].copy()

    for dc in ("Created on","Planned Start Date","Due date","Started on","Completed on"):
        if dc in df.columns:
            df[dc] = df[dc].map(_norm_date_any)
    for bc in ("IsOpen","IsOverdue","IsScheduled","IsCompleted","IsOld"):
        if bc in df.columns:
            df[bc] = coerce_bool(df[bc])
    for c in [x for x in ["ID","Title","Description","Asset","Status","Assigned to","Teams Assigned to","Completed by","Location"] if x in df.columns]:
        df[c] = df[c].astype("string").str.strip()
    if "ID" in df.columns:
        df["ID"] = df["ID"].astype("string").str.strip()

    for catcol in ("Location","Assigned to","Asset"):
        if catcol in df.columns:
            df[catcol] = df[catcol].astype("category")
    return df

@st.cache_data(show_spinner=False)
def load_service_report_df(xlsx_mtime: float, xlsx_path_str: str):
    xlsx_path = Path(xlsx_path_str)

    for nm in SHEET_SERVICE_CANDIDATES:
        if _parquet_fresh(nm, xlsx_path):
            raw = _read_parquet_columns(nm, columns=None)
        else:
            try:
                _rebuild_parquet_from_excel(get_local_xlsx_bytes_cached(xlsx_path_str, xlsx_mtime), [nm])
                if _parquet_fresh(nm, xlsx_path):
                    raw = _read_parquet_columns(nm, columns=None)
                else:
                    continue
            except Exception:
                continue

        raw.columns = [str(c).strip() for c in raw.columns]
        canon = _canonize_headers(raw.copy(), SERVICE_REPORT_CANON)
        if "Date" in canon.columns: canon["Date"] = canon["Date"].map(_norm_date_any)
        if "Due Date" in canon.columns: canon["Due Date"] = canon["Due Date"].map(_norm_date_any)

        for col, newcol in [("Schedule","__Schedule_num"), ("Remaining","__Remaining_num"), ("Percent Remaining","__PctRemain_num")]:
            if col in canon.columns:
                canon[newcol] = pd.to_numeric(canon[col].astype(str).str.replace("%","", regex=False), errors="coerce")
            else:
                canon[newcol] = pd.NA
        if "__PctRemain_num" in canon.columns:
            pr = pd.to_numeric(canon["__PctRemain_num"], errors="coerce")
            canon["__PctRemain_num"] = pr.where((pr.isna()) | (pr <= 1.0), pr/100.0)

        canon["__MeterType_norm"] = canon.get("Meter Type", pd.Series([], dtype=str)).astype(str).str.strip().str.lower() if "Meter Type" in canon.columns else ""
        canon["__Due_dt"] = pd.to_datetime(canon["Due Date"], errors="coerce") if "Due Date" in canon.columns else pd.NaT
        for c in [x for x in ["WO_ID","Title","Service","Asset","Location","User","Notes","Status","MReading"] if x in canon.columns]:
            canon[c] = canon[c].astype("string").str.strip()
        return raw, canon, nm

    return None, None, None

@st.cache_data(show_spinner=False)
def load_service_history_df(xlsx_mtime: float, xlsx_path_str: str):
    xlsx_path = Path(xlsx_path_str)
    last_err = None
    for nm in SHEET_WO_SERVICE_CANDS:
        try:
            if _parquet_fresh(nm, xlsx_path):
                df = _read_parquet_columns(nm, columns=None)
            else:
                _rebuild_parquet_from_excel(get_local_xlsx_bytes_cached(xlsx_path_str, xlsx_mtime), [nm])
                df = _read_parquet_columns(nm, columns=None)

            df.columns = [str(c).strip() for c in df.columns]
            df = _canonize_headers(df, SERVICE_HISTORY_CANON)
            if "Date" in df.columns: df["Date"] = df["Date"].map(_norm_date_any)
            for c in [x for x in ["WO_ID","Title","Service","Asset","Location2","User","Notes","Status","MReading","MHours"] if x in df.columns]:
                df[c] = df[c].astype("string").str.strip()
            keep = [c for c in ["Date","WO_ID","Title","Service","MReading","MHours","Asset","User","Location2","Notes","Status"] if c in df.columns]
            df = df[keep].copy() if keep else df
            return df, nm
        except Exception as e:
            last_err = e
            continue
    return None, f"{last_err}" if last_err else None

@st.cache_data(show_spinner=False)
def load_users_sheet(xlsx_mtime: float, xlsx_path_str: str) -> list[str] | None:
    xlsx_path = Path(xlsx_path_str)
    for name in SHEET_USERS_CANDIDATES:
        try:
            if _parquet_fresh(name, xlsx_path):
                dfu = _read_parquet_columns(name, columns=None)
            else:
                _rebuild_parquet_from_excel(get_local_xlsx_bytes_cached(xlsx_path_str, xlsx_mtime), [name])
                dfu = _read_parquet_columns(name, columns=None)
            cols_low = {c.lower(): c for c in dfu.columns}
            col = cols_low.get("user")
            if not col:
                continue
            users = [u.strip() for u in dfu[col].astype(str).tolist() if str(u).strip()]
            users = sorted(dict.fromkeys(users))
            return users
        except Exception:
            pass
    return None

# ---------- Service Procedure Filter helpers ----------
def normalize_pn(pn) -> str:
    if pd.isna(pn): return ""
    return "".join(ch for ch in str(pn).upper() if ch.isalnum())

_SPLIT_RE = re.compile(r"[,\;/\s]+")

def _token_norms_from_text(s: str):
    if s is None: return []
    toks = []
    for piece in _SPLIT_RE.split(str(s).upper()):
        piece = piece.strip()
        if not piece:
            continue
        n = normalize_pn(piece)
        if n and any(ch.isdigit() for ch in n):
            toks.append(n)
    return toks

def _semi_join(tokens):
    uniq, seen = [], set()
    for t in tokens:
        if t not in seen:
            uniq.append(t); seen.add(t)
    return ";" + ";".join(uniq) + ";" if uniq else ";"

@st.cache_data(ttl=300, show_spinner=False)
def load_procedure_workbook(xlsx_mtime: float, proc_path_str: str):
    proc_path = Path(proc_path_str)
    need_proc = {COL_SERIAL, COL_SERVICE, COL_TASKNO, COL_TASK}
    need_ctrl = {COL_ASSET, COL_SERIAL, COL_LOC_LIST}
    need_inv  = {INV_COL_PN, INV_COL_QTY, INV_COL_LOC, INV_COL_AREA, INV_COL_LOC2}

    # Ensure parquet present/fresh
    for sh in (SHEET_PROC, SHEET_CTRL, SHEET_INV, SHEET_XREF):
        if not _parquet_fresh(sh, proc_path):
            _rebuild_parquet_from_excel(get_local_xlsx_bytes_cached(proc_path_str, xlsx_mtime), [sh])

    try:
        df_proc = _read_parquet_columns(SHEET_PROC, columns=None)
        df_ctrl = _read_parquet_columns(SHEET_CTRL, columns=None)
        df_inv  = _read_parquet_columns(SHEET_INV,  columns=None)
    except Exception as e:
        raise RuntimeError(f"Failed reading procedures workbook sheets: {e}")

    try:
        df_xref = _read_parquet_columns(SHEET_XREF, columns=None)
    except Exception:
        df_xref = pd.DataFrame()

    df_proc.columns = [str(c).strip() for c in df_proc.columns]
    df_ctrl.columns = [str(c).strip() for c in df_ctrl.columns]
    df_inv.columns  = [str(c).strip() for c in df_inv.columns]
    df_xref.columns = [str(c).strip() for c in df_xref.columns]

    errors = []
    if not need_proc.issubset(df_proc.columns): errors.append("Service Procedures missing required columns.")
    if not need_ctrl.issubset(df_ctrl.columns): errors.append("Controls missing Asset/Serial/Locations.")
    if not need_inv.issubset(df_inv.columns):   errors.append("Parts_Master missing {Part Numbers, Quantity in Stock, Location, Area, Location2}.")
    if errors:
        raise RuntimeError(" ".join(errors))

    # Precompute inventory helpers
    inv_df = df_inv.copy()
    name_col = next((c for c in INV_NAME_CANDIDATES if c in inv_df.columns), None)

    def _clean_space(s):
        if s is None: return ""
        return str(s).replace("\u00A0", "").replace("\t", "").strip()

    def _build_tok_name(row) -> str:
        if not name_col: return ";"
        toks = _token_norms_from_text(row.get(name_col, ""))
        return _semi_join(toks)

    def _build_tok_pn(row) -> str:
        toks = _token_norms_from_text(row.get(INV_COL_PN, ""))
        return _semi_join(toks)

    inv_df["_TOK_NAME_SEMI"] = inv_df.apply(_build_tok_name, axis=1) if name_col else ";"
    inv_df["_TOK_PN_SEMI"]   = inv_df.apply(_build_tok_pn,   axis=1)
    inv_df["_LOC2_CLEAN"]    = inv_df[INV_COL_LOC2].map(_clean_space)
    inv_df["_QTY_NUM"]       = pd.to_numeric(inv_df[INV_COL_QTY], errors="coerce").fillna(0)

    return df_proc, df_ctrl, inv_df, df_xref, name_col

def excel_like_first_match(inv_df: pd.DataFrame, pn_norm: str, loc_val: str):
    if pn_norm == "":
        return None
    loc_clean = (str(loc_val) or "").replace("\u00A0", "").replace("\t", "").strip()
    base = (inv_df["_LOC2_CLEAN"] == loc_clean) & (inv_df["_QTY_NUM"] > 0)
    # 1) Name-first match
    if "_TOK_NAME_SEMI" in inv_df.columns:
        m1 = inv_df["_TOK_NAME_SEMI"].str.contains(";" + pn_norm + ";", regex=False, na=False)
        sub1 = inv_df.loc[base & m1]
        if not sub1.empty:
            return sub1.iloc[0]
    # 2) Fallback: Part Numbers
    m2 = inv_df["_TOK_PN_SEMI"].str.contains(";" + pn_norm + ";", regex=False, na=False)
    sub2 = inv_df.loc[base & m2]
    if not sub2.empty:
        return sub2.iloc[0]
    return None

def inv_text_from_row(row):
    if row is None:
        return "No Stock"
    loc  = str(row.get(INV_COL_LOC, "")).strip()
    area = str(row.get(INV_COL_AREA, "")).strip()
    qty  = row.get(INV_COL_QTY)
    try:
        q = int(float(qty))
    except Exception:
        q = qty
    parts = []
    if loc:  parts.append(loc)
    if area: parts.append(area)
    if q is not None and str(q) != "nan":
        parts.append(f"Qty - {q}")
    return "; ".join(parts) if parts else "No Stock"

def unpivot_to_task_then_parts(df_proc_filtered: pd.DataFrame) -> pd.DataFrame:
    cols = list(df_proc_filtered.columns)
    part_cols = [c for c in cols if c.startswith(PART_PREFIXES)]

    # Header rows
    hdr = df_proc_filtered[[COL_TASKNO, COL_TASK]].drop_duplicates().copy()
    hdr["Part Number"] = None
    hdr["Part Type"]   = None
    hdr["Qty"]         = None
    hdr["RowKind"]     = "Task"
    hdr["SfxKey"]      = 0

    if not part_cols:
        out = hdr.copy()
        out["__TaskNum"] = pd.to_numeric(out[COL_TASKNO], errors="coerce").fillna(0)
        out = out.sort_values(by=["__TaskNum", "RowKind", "SfxKey"]).drop(columns="__TaskNum")
        return out[[COL_TASKNO, COL_TASK, "Part Number", "Part Type", "Qty"]].reset_index(drop=True)

    long = df_proc_filtered.melt(
        id_vars=[c for c in cols if c not in part_cols],
        value_vars=part_cols, var_name="Attr", value_name="Val"
    )
    long["Kind"] = long["Attr"].str.extract(r"^(.*)_", expand=False)
    long["Sfx"]  = long["Attr"].str.extract(r"_(.*)$",  expand=False)
    long = long.drop(columns=["Attr"])

    def first_non_null(x: pd.Series):
        x = x.dropna()
        return x.iloc[0] if not x.empty else None

    idx_cols = [c for c in [COL_TASKNO, COL_TASK, COL_SERVICE, COL_SERIAL, "Sfx"] if c in long.columns]
    wide = long.pivot_table(index=idx_cols, columns="Kind", values="Val",
                            aggfunc=first_non_null).reset_index()

    has_pt  = "Part Type" in wide.columns
    has_dsc = "Description" in wide.columns
    if has_pt and has_dsc:
        wide["Part Type (Display)"] = wide["Part Type"].where(
            wide["Part Type"].astype(str).str.strip().ne(""),
            wide["Description"]
        )
    elif has_pt:
        wide["Part Type (Display)"] = wide["Part Type"]
    elif has_dsc:
        wide["Part Type (Display)"] = wide["Description"]
    else:
        wide["Part Type (Display)"] = None

    if "Qty" in wide.columns:
        wide["Qty"] = pd.to_numeric(wide["Qty"], errors="coerce")

    sel_cols = [c for c in [COL_TASKNO, COL_TASK, "Part Number", "Part Type (Display)", "Qty", "Sfx"] if c in wide.columns]
    parts = wide[sel_cols].rename(columns={"Part Type (Display)": "Part Type"}).copy()
    parts["RowKind"] = "Part"
    parts["SfxKey"]  = pd.to_numeric(parts.get("Sfx"), errors="coerce").fillna(9999).astype(int)
    if "Sfx" in parts.columns:
        parts.drop(columns=["Sfx"], inplace=True)

    # Align and combine
    final_cols = [COL_TASKNO, COL_TASK, "Part Number", "Part Type", "Qty", "RowKind", "SfxKey"]

    def coerce_cols(df: pd.DataFrame) -> pd.DataFrame:
        df = df.loc[:, ~pd.Index(df.columns).duplicated(keep="first")]
        for c in final_cols:
            if c not in df.columns:
                df[c] = None
        return df[final_cols]

    hdr   = coerce_cols(hdr)
    parts = coerce_cols(parts)

    combined = pd.concat([hdr, parts], ignore_index=True, sort=False)
    combined["__TaskNum"] = pd.to_numeric(combined[COL_TASKNO], errors="coerce").fillna(0)
    combined["__rk"]      = combined["RowKind"].map({"Task": 0, "Part": 1}).fillna(1)
    combined = combined.sort_values(by=["__TaskNum", "__rk", "SfxKey"]).drop(columns=["__TaskNum", "__rk"])

    combined.loc[combined["RowKind"] == "Part", COL_TASKNO] = None
    return combined[[COL_TASKNO, COL_TASK, "Part Number", "Part Type", "Qty"]].reset_index(drop=True)

def proc_to_docx_bytes(df: pd.DataFrame, *, asset: str, serial: str, service: str, location: str) -> bytes:
    from docx import Document
    from docx.shared import Inches, Pt
    from docx.enum.section import WD_ORIENT
    from docx.oxml import OxmlElement
    from docx.oxml.ns import qn

    def _repeat_header(row):
        tr = row._tr
        trPr = tr.get_or_add_trPr()
        tblHeader = OxmlElement('w:tblHeader')
        trPr.append(tblHeader)

    doc = Document()
    section = doc.sections[0]
    section.orientation = WD_ORIENT.LANDSCAPE
    section.page_width, section.page_height = section.page_height, section.page_width
    for side in ("left_margin", "right_margin", "top_margin", "bottom_margin"):
        setattr(section, side, Inches(0.5))

    doc.styles['Normal'].font.name = "Calibri"
    doc.styles['Normal'].font.size = Pt(10)

    title = doc.add_paragraph()
    run = title.add_run(
        f"Service Procedure Filter ‚Äî Asset: {asset}  (Serial: {serial})  |  Service: {service}  |  Location: {location}"
    )
    run.bold = True
    doc.add_paragraph("")

    cols = list(df.columns)
    table = doc.add_table(rows=1, cols=len(cols))
    table.autofit = True
    hdr_cells = table.rows[0].cells
    for i, c in enumerate(cols):
        p = hdr_cells[i].paragraphs[0]
        r = p.add_run(str(c)); r.bold = True
        tcPr = hdr_cells[i]._tc.get_or_add_tcPr()
        shd = OxmlElement('w:shd'); shd.set(qn('w:fill'), "DDDDDD"); tcPr.append(shd)
    _repeat_header(table.rows[0])

    for _, row in df.iterrows():
        cells = table.add_row().cells
        for i, c in enumerate(cols):
            val = row.get(c, "")
            cells[i].text = "" if pd.isna(val) else str(val)

    width_map = {
        "Task #": 0.6, "Task": 1.8,
        "Part Number": 1.1, "Part Type": 1.9, "Qty": 0.6,
        "InStk": 1.5, "Donaldson Interchange": 1.3, "In Stock": 1.5,
    }
    for i, c in enumerate(cols):
        w = width_map.get(c)
        if w:
            for cell in [r.cells[i] for r in table.rows]:
                cell.width = Inches(w)

    out = io.BytesIO(); doc.save(out); out.seek(0); return out.read()

# ---------- App ----------
st.sidebar.caption(f"SPF Work Orders ‚Äî v{APP_VERSION}")

cfg = load_config()
cfg = to_plain(cfg)

# Auth
cookie_cfg = cfg.get("cookie", {})
auth = stauth.Authenticate(
    cfg.get("credentials", {}),
    cookie_cfg.get("name", "spf_workorders_portal"),
    cookie_cfg.get("key", "super_secret_key"),
    cookie_cfg.get("expiry_days", 7),
)
name, auth_status, username = auth.login("Login", "main")

if auth_status is False:
    st.error("Username/password is incorrect")
elif auth_status is None:
    st.info("Please log in.")
else:
    auth.logout("Logout", "sidebar")
    st.sidebar.success(f"Logged in as {name}")

    updated = get_data_last_updated_local(cfg)
    if updated: st.sidebar.caption(updated)

    if st.sidebar.button("üîÑ Refresh data"):
        st.cache_data.clear()
        st.rerun()

    page = st.sidebar.radio(
        "Page",
        ["üîé Asset History", "üìã Work Orders", "üßæ Service Report", "üìö Service History", "üõ†Ô∏è Service Procedure Filter"],
        index=1
    )

    # Load workbook bytes + expose Parquet cache tools
    xlsx_path = _xlsx_path_from_cfg(cfg)
    try:
        xlsx_bytes = get_local_xlsx_bytes(cfg)
    except Exception as e:
        st.error(f"Could not load Excel: {e}")
        st.stop()

    parquet_button_and_refresh(xlsx_path, xlsx_bytes)
    xmtime = _mtime(xlsx_path)

    # Also get path for procedures (may be same file)
    proc_bytes, proc_path = get_local_proc_xlsx_bytes(cfg)
    pxmtime = _mtime(proc_path)

    # Access control: Locations from Asset_Master
    try:
        df_am = load_asset_master_df(xmtime, str(xlsx_path))
    except Exception as e:
        st.error(f"Failed to read Asset_Master: {e}")
        st.stop()

    username_ci = str(username).casefold()
    admins_ci = {str(u).casefold() for u in (cfg.get("access", {}).get("admin_usernames", []) or [])}
    is_admin = username_ci in admins_ci
    ul_raw = (cfg.get("access", {}).get("user_locations", {}) or {})
    ul_map_ci = {str(k).casefold(): v for k, v in ul_raw.items()}
    allowed_cfg = ul_map_ci.get(username_ci, [])
    if isinstance(allowed_cfg, str): allowed_cfg = [allowed_cfg]
    allowed_cfg = [a for a in (allowed_cfg or [])]
    star = any(str(a).strip() == "*" for a in allowed_cfg)

    all_locations = sorted(df_am["Location"].dropna().unique().tolist())
    allowed_locations = set(all_locations) if (is_admin or star) else {loc for loc in all_locations if loc in set(allowed_cfg)}
    allowed_norms = {_norm_key(x) for x in allowed_locations}

    # ========= Asset History =========
    if page == "üîé Asset History":
        st.markdown("### Asset History")
        c1, c2 = st.columns([2, 3])
        with c1:
            loc_options = sorted(allowed_locations)
            chosen_loc = st.selectbox("Location", options=loc_options, index=0, label_visibility="collapsed")
        with c2:
            assets_for_loc = sorted(df_am.loc[df_am["Location"] == chosen_loc, "ASSET"].dropna().unique().tolist())
            chosen_asset = st.selectbox("Asset", options=assets_for_loc, index=0 if assets_for_loc else None, label_visibility="collapsed")

        if not assets_for_loc:
            st.info("No assets for this Location.")
            st.stop()

        try:
            df_all = load_workorders_df(xmtime, str(xlsx_path))
        except Exception as e:
            st.error(f"Failed to read Workorders (history): {e}")
            st.stop()

        df = df_all[(df_all["Location"] == chosen_loc) & (df_all["ASSET"] == chosen_asset)].copy()
        if "QUANTITY RECEIVED" in df.columns and "P/N" in df.columns:
            qnum = pd.to_numeric(df["QUANTITY RECEIVED"], errors="coerce")
            is_part = df["P/N"].astype(str).str.strip().ne("")
            df = df[~(is_part & qnum.notna() & (qnum <= 0))].copy()

        df["__row"] = range(len(df))
        if OPTIONAL_SORT_COL in df.columns:
            df["__sort_key"] = pd.to_numeric(df[OPTIONAL_SORT_COL], errors="coerce").fillna(1).astype(int)
        else:
            df["__sort_key"] = 1
        df.sort_values(by=["WORKORDER","__sort_key","__row"], ascending=[True, True, True], inplace=True)
        df.loc[df["__sort_key"].isin([2, 3]), "WORKORDER"] = ""
        drop_cols = ["__row","__sort_key", OPTIONAL_SORT_COL]
        df_out = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

        st.dataframe(df_out, use_container_width=True, hide_index=True)

        c1, c2, _ = st.columns([1, 1, 6])
        with c1:
            st.download_button(
                label="‚¨áÔ∏è Excel (.xlsx)",
                data=to_xlsx_bytes(df_out, sheet="Workorders"),
                file_name=f"WorkOrders_{chosen_loc}_{chosen_asset}.xlsx".replace(" ", "_"),
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
        with c2:
            st.download_button(
                label="‚¨áÔ∏è Word (.docx)",
                data=to_docx_bytes(df_out, title=f"Work Orders ‚Äî {chosen_loc} ‚Äî {chosen_asset}"),
                file_name=f"WorkOrders_{chosen_loc}_{chosen_asset}.docx".replace(" ", "_"),
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            )
        st.stop()

    # ========= Work Orders =========
    if page == "üìã Work Orders":
        st.markdown("### Work Orders ‚Äî Filtered Views (flags from workbook)")

        try:
            df_master = load_wo_master_df(xmtime, str(xlsx_path))
        except Exception as e:
            st.error(f"Failed to read '{SHEET_WO_MASTER}': {e}")
            st.stop()

        opt_users = load_users_sheet(xmtime, str(xlsx_path))
        df_master = df_master[df_master["Location"].isin(allowed_locations)].copy()
        total_in_scope = len(df_master)

        c1, c2, c3, c4 = st.columns([2, 2, 2, 3])
        with c1:
            loc_values = sorted(df_master["Location"].dropna().unique().tolist())
            loc_all_label = f"¬´ All my locations ({len(loc_values)}) ¬ª"
            chosen_loc = st.selectbox("Location", options=[loc_all_label] + loc_values, index=0, label_visibility="collapsed")

        df_scope = df_master if chosen_loc == loc_all_label else df_master[df_master["Location"] == chosen_loc].copy()

        with c2:
            if opt_users is not None:
                user_choices = ["‚Äî Any user ‚Äî"] + opt_users
            else:
                derived_users = sorted([u for u in df_scope.get("Assigned to", pd.Series([], dtype=str)).dropna().astype(str).str.strip().unique().tolist() if u])
                user_choices = ["‚Äî Any user ‚Äî"] + derived_users
            sel_user = st.selectbox("Assigned user", options=user_choices, index=0, label_visibility="collapsed")

        with c3:
            raw_teams = df_scope.get("Teams Assigned to", pd.Series([], dtype=str)).fillna("").astype(str)
            token_set = set()
            for v in raw_teams:
                for t in re.split(r"[;,]", v):
                    t = t.strip()
                    if t:
                        token_set.add(t)
            team_opts = ["‚Äî Any team ‚Äî"] + sorted(token_set)
            sel_team = st.selectbox("Team", options=team_opts, index=0, label_visibility="collapsed")

        with c4:
            view = st.radio("View", ["All","Open","Overdue","Scheduled (Planning)","Completed","Old"],
                            horizontal=True, index=1 if "IsOpen" in df_scope.columns else 0)

        if sel_user != "‚Äî Any user ‚Äî":
            df_scope = df_scope[df_scope["Assigned to"].astype(str).str.strip() == sel_user].copy()
            if df_scope.empty:
                st.warning("User not found at this location (or no work orders match).")
                st.dataframe(df_scope, use_container_width=True, hide_index=True)
                st.stop()

        if sel_team != "‚Äî Any team ‚Äî":
            def team_hit(s: str) -> bool:
                if not s: return False
                parts = {p.strip() for p in re.split(r"[;,]", str(s)) if p.strip()}
                return sel_team.strip() in parts
            df_scope = df_scope[df_scope["Teams Assigned to"].fillna("").astype(str).map(team_hit)].copy()

        def pick_view(df_in: pd.DataFrame) -> pd.DataFrame:
            if view == "All" or not {"IsOpen","IsOverdue","IsScheduled","IsCompleted","IsOld"}.issubset(df_in.columns):
                return df_in
            col = {"Open":"IsOpen","Overdue":"IsOverdue","Scheduled (Planning)":"IsScheduled","Completed":"IsCompleted","Old":"IsOld"}[view]
            return df_in[df_in[col]].copy()

        df_view = pick_view(df_scope)

        def present(cols: list[str]) -> list[str]:
            return [c for c in cols if c in df_view.columns]

        cols_all       = present(["ID","Title","Description","Asset","Created on","Planned Start Date","Due date","Started on","Completed on","Assigned to","Teams Assigned to","Location"])
        cols_open      = present(["ID","Title","Description","Asset","Created on","Due date","Assigned to","Teams Assigned to","Location"])
        cols_overdue   = present(["ID","Title","Description","Asset","Due date","Assigned to","Teams Assigned to","Location"])
        cols_sched     = present(["ID","Title","Description","Asset","Planned Start Date","Due date","Assigned to","Teams Assigned to","Location"])
        cols_completed = present(["ID","Title","Description","Asset","Completed on","Assigned to","Teams Assigned to","Location"])
        cols_old       = present(["ID","Title","Description","Asset","Created on","Due date","Completed on","Assigned to","Teams Assigned to","Location"])

        if view == "Open":
            use_cols = cols_open or df_view.columns.tolist()
            sort_keys = [k for k in ["Due date","Created on","Title"] if k in df_view.columns]
        elif view == "Overdue":
            use_cols = cols_overdue or df_view.columns.tolist()
            sort_keys = [k for k in ["Due date","Title"] if k in df_view.columns]
        elif view == "Scheduled (Planning)":
            use_cols = cols_sched or df_view.columns.tolist()
            sort_keys = [k for k in ["Planned Start Date","Due date","Title"] if k in df_view.columns]
        elif view == "Completed":
            use_cols = cols_completed or df_view.columns.tolist()
            sort_keys = [k for k in ["Completed on","Title"] if k in df_view.columns]
        elif view == "Old":
            use_cols = cols_old or df_view.columns.tolist()
            sort_keys = [k for k in ["Created on","Due date","Title"] if k in df_view.columns]
        else:
            use_cols = cols_all or df_view.columns.tolist()
            sort_keys = [k for k in ["Completed on","Due date","Planned Start Date","Created on","Title"] if k in df_view.columns]

        if sort_keys: df_view = df_view.sort_values(by=sort_keys, na_position="last")

        st.caption(f"In scope: {total_in_scope}  ‚Ä¢  After location/user/team filters: {len(df_scope)}  ‚Ä¢  Showing ({view}): {len(df_view)}")
        st.dataframe(df_view[use_cols], use_container_width=True, hide_index=True)

        with st.expander("üóìÔ∏è 7-day Scheduled Planner (printable)", expanded=False):
            df_sched = df_scope[df_scope.get("IsScheduled", False)].copy()
            if df_sched.empty:
                st.info("No scheduled items.")
            else:
                df_sched["Planned Start Date"] = pd.to_datetime(df_sched["Planned Start Date"], errors="coerce")
                start_base = pd.to_datetime(datetime.now().date())
                dates = [start_base + timedelta(days=i) for i in range(7)]
                labels = [d.strftime("%a %m/%d") for d in dates]
                cols = st.columns(7)
                for i, d in enumerate(dates):
                    with cols[i]:
                        st.markdown(f"**{labels[i]}**")
                        day_rows = df_sched[df_sched["Planned Start Date"].dt.date == d.date()]
                        if day_rows.empty:
                            st.caption("‚Äî")
                        else:
                            show_cols = [c for c in ["ID","Title","Description","Asset","Assigned to","Location"] if c in day_rows.columns]
                            st.dataframe(day_rows[show_cols], use_container_width=True, hide_index=True)

        c1, c2, _ = st.columns([1, 1, 6])
        with c1:
            st.download_button("‚¨áÔ∏è Excel (.xlsx)", data=to_xlsx_bytes(df_view[use_cols], sheet="WorkOrders"),
                               file_name=f"WorkOrders_{view.replace(' ','_')}.xlsx",
                               mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        with c2:
            st.download_button("‚¨áÔ∏è Word (.docx)", data=to_docx_bytes(df_view[use_cols], title=f"Work Orders ‚Äî {view}"),
                               file_name=f"WorkOrders_{view.replace(' ','_')}.docx",
                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
        st.stop()

    # ========= Service Report =========
    if page == "üßæ Service Report":
        st.markdown("### Service Report")

        raw_sr, canon_sr, source_sheet = load_service_report_df(xmtime, str(xlsx_path))
        if raw_sr is None:
            st.warning("No 'Service Report' sheet found.")
            st.stop()

        # Location filter (shorter)
        loc_col = None
        for c in raw_sr.columns:
            if c.strip().lower() in {"location","ns location","location2"}:
                loc_col = c; break

        if loc_col:
            loc_values_all = raw_sr[loc_col].astype(str)
            loc_candidates = sorted({v for v in loc_values_all if _norm_key(v) in allowed_norms})
        else:
            loc_candidates = sorted(allowed_locations)

        c_loc, _ = st.columns([3, 7])
        with c_loc:
            loc_all_label = f"¬´ All my locations ({len(loc_candidates)}) ¬ª" if loc_candidates else "¬´ All my locations ¬ª"
            chosen_loc = st.selectbox("Location", options=[loc_all_label] + loc_candidates if loc_candidates else [loc_all_label],
                                      index=0, label_visibility="collapsed")

        if loc_col and chosen_loc != loc_all_label:
            mask_norm = loc_values_all.map(_norm_key) == _norm_key(chosen_loc)
            raw_show = raw_sr[mask_norm].copy()
            if "Location" in canon_sr.columns:
                canon_in_scope = canon_sr[canon_sr["Location"].map(_norm_key) == _norm_key(chosen_loc)].copy()
            else:
                canon_in_scope = canon_sr.copy()
        else:
            raw_show = raw_sr.copy()
            canon_in_scope = canon_sr.copy()

        t_report, t_due, t_over = st.tabs(["Report", "Coming Due", "Overdue"])

        # --- Report (as-is, minus Schedule/Today) with Date normalized to date-only ---
        with t_report:
            drop_cols = [c for c in ["Schedule","Today"] if c in raw_show.columns]
            show_rep = raw_show.drop(columns=drop_cols) if drop_cols else raw_show
            # Normalize a column explicitly named 'Date' to YYYY-MM-DD for display only
            if "Date" in show_rep.columns:
                show_rep = show_rep.copy()
                show_rep["Date"] = show_rep["Date"].map(_norm_date_any)
            st.caption(f"Source: {source_sheet}  ‚Ä¢  Rows: {len(show_rep)}  ‚Ä¢  No filters other than Location.")
            st.dataframe(show_rep, use_container_width=True, hide_index=True)
            c1, c2, _ = st.columns([1,1,6])
            with c1:
                st.download_button("‚¨áÔ∏è Excel (.xlsx)", data=to_xlsx_bytes(show_rep, sheet="Service_Report"),
                                   file_name="Service_Report.xlsx",
                                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            with c2:
                st.download_button("‚¨áÔ∏è Word (.docx)", data=to_docx_bytes(show_rep, title="Service Report ‚Äî As Is"),
                                   file_name="Service_Report.docx",
                                   mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")

        # Helper: compute Coming Due / Overdue frames (vectorized thresholds)
        def _due_frames(df_can: pd.DataFrame):
            if df_can is None or df_can.empty:
                return pd.DataFrame(), pd.DataFrame()
            df = df_can.copy()
            # 5% threshold if meter type mentions miles, else 10%
            mt = df.get("__MeterType_norm")
            thr_series = pd.Series(0.10, index=df.index)
            if mt is not None:
                thr_series = thr_series.where(~mt.astype(str).str.contains("mile"), 0.05)

            conds = []
            condA = (
                df["__Schedule_num"].notna() &
                (pd.to_numeric(df["__Schedule_num"], errors="coerce") > 0) &
                df["__Remaining_num"].notna() &
                (pd.to_numeric(df["__Remaining_num"], errors="coerce") >= 0)
            )
            if condA.any():
                condA2 = pd.to_numeric(df["__Remaining_num"], errors="coerce") <= (pd.to_numeric(df["__Schedule_num"], errors="coerce") * thr_series)
                conds.append(condA & condA2)
            if "__PctRemain_num" in df.columns:
                condB = df["__PctRemain_num"].notna()
                if condB.any():
                    condB2 = pd.to_numeric(df["__PctRemain_num"], errors="coerce") <= thr_series
                    conds.append(condB & condB2)
            coming_due_mask = pd.Series(False, index=df.index)
            for c in conds: coming_due_mask |= c
            coming_due = df[coming_due_mask].copy()

            today = pd.Timestamp.today().normalize()
            overdue_mask = pd.Series(False, index=df.index)
            if "__Remaining_num" in df.columns:
                overdue_mask |= (pd.to_numeric(df["__Remaining_num"], errors="coerce") < 0)
            if "__Due_dt" in df.columns:
                overdue_mask |= (pd.to_datetime(df["__Due_dt"], errors="coerce") < today)
            overdue = df[overdue_mask].copy()
            return coming_due, overdue

        coming_due_df, overdue_df = _due_frames(canon_in_scope)

        def reorder_with_optional(df: pd.DataFrame, pref: list[str]) -> pd.DataFrame:
            cols = [c for c in pref if c in df.columns]
            rest = [c for c in df.columns if c not in cols]
            return df[cols + rest]

        with t_due:
            st.caption("Coming Due = Remaining ‚â§ 10% of Schedule (or ‚â§ 5% if Meter Type contains 'miles').")
            if coming_due_df.empty:
                st.info("No items are coming due based on the available columns.")
            else:
                base = ["WO_ID","Title","Service","Asset","Location","Date","User","Status",
                        "Schedule","Remaining","Percent Remaining","Meter Type","Due Date","Notes","MReading"]
                show = coming_due_df[[c for c in base if c in coming_due_df.columns]].copy()
                show = show.sort_values(by=[c for c in ["Due Date","Percent Remaining","Remaining"] if c in show.columns], na_position="last")
                st.dataframe(show, use_container_width=True, hide_index=True)
                c1, c2, _ = st.columns([1,1,6])
                with c1:
                    st.download_button("‚¨áÔ∏è Excel (.xlsx)", data=to_xlsx_bytes(show, sheet="Service_Coming_Due"),
                                       file_name="Service_Coming_Due.xlsx",
                                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                with c2:
                    st.download_button("‚¨áÔ∏è Word (.docx)", data=to_docx_bytes(show, title="Service ‚Äî Coming Due"),
                                       file_name="Service_Coming_Due.docx",
                                       mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")

        with t_over:
            st.caption("Overdue = Remaining < 0, or Due Date earlier than today.")
            if overdue_df.empty:
                st.info("No overdue items found.")
            else:
                base = ["WO_ID","Title","Service","Asset","Location","Date","User","Status",
                        "Schedule","MReading","Remaining","Percent Remaining","Meter Type","Due Date","Notes"]
                show = overdue_df[[c for c in base if c in overdue_df.columns]].copy()
                show = reorder_with_optional(show, base)
                show = show.sort_values(by=[c for c in ["Due Date","Remaining"] if c in show.columns], na_position="last")
                st.dataframe(show, use_container_width=True, hide_index=True)
                c1, c2, _ = st.columns([1,1,6])
                with c1:
                    st.download_button("‚¨áÔ∏è Excel (.xlsx)", data=to_xlsx_bytes(show, sheet="Service_Overdue"),
                                       file_name="Service_Overdue.xlsx",
                                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                with c2:
                    st.download_button("‚¨áÔ∏è Word (.docx)", data=to_docx_bytes(show, title="Service ‚Äî Overdue"),
                                       file_name="Service_Overdue.docx",
                                       mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
        st.stop()

    # ========= Service History =========
    if page == "üìö Service History":
        st.markdown("### Service History")

        df_hist, used_sheet_or_err = load_service_history_df(xmtime, str(xlsx_path))
        if df_hist is None or df_hist.empty:
            msg = used_sheet_or_err or "unknown error"
            st.warning(f"No Service History data found. Tried: {SHEET_WO_SERVICE_CANDS}. Last error: {msg}")
            st.stop()

        # Normalize and restrict to allowed by Location2
        if "Location2" in df_hist.columns:
            df_hist["__LocNorm"] = df_hist["Location2"].map(_norm_key)
            df_hist = df_hist[df_hist["__LocNorm"].isin(allowed_norms)].copy()

        # Filters: Location + Asset (compact)
        c1, c2 = st.columns([2, 3])
        with c1:
            if "Location2" in df_hist.columns:
                loc_values = sorted(df_hist["Location2"].dropna().unique().tolist())
            else:
                loc_values = []
            loc_all_label = f"¬´ All my locations ({len(loc_values)}) ¬ª" if loc_values else "¬´ All my locations ¬ª"
            chosen_loc = st.selectbox("Location", options=[loc_all_label] + loc_values if loc_values else [loc_all_label],
                                      index=0, label_visibility="collapsed")

        if chosen_loc != loc_all_label and "Location2" in df_hist.columns:
            scope = df_hist[df_hist["Location2"].map(_norm_key) == _norm_key(chosen_loc)].copy()
        else:
            scope = df_hist.copy()

        with c2:
            assets = sorted([a for a in scope.get("Asset", pd.Series([], dtype=str)).dropna().astype(str).str.strip().unique().tolist() if a])
            sel_asset = st.selectbox("Asset", options=assets, index=0 if assets else None, label_visibility="collapsed")

        if not assets:
            st.info("No assets available in this Location.")
            st.stop()

        scope = scope[scope["Asset"] == sel_asset] if "Asset" in scope.columns else scope

        if "Date" in scope.columns:
            scope = scope.copy()
            scope["__Date_dt"] = pd.to_datetime(scope["Date"], errors="coerce")
            scope = scope.sort_values(by="__Date_dt", ascending=False, na_position="last").drop(columns="__Date_dt")

        # Show MReading right after Service
        col_order = [c for c in ["Date","WO_ID","Title","Service","MReading","MHours","Asset","User","Location2","Notes","Status"] if c in scope.columns]
        st.caption(f"Sheet used: {used_sheet_or_err} ‚Ä¢ Rows: {len(scope)}")
        st.dataframe(scope[col_order] if col_order else scope, use_container_width=True, hide_index=True)

        c1, c2, _ = st.columns([1,1,6])
        with c1:
            st.download_button(
                "‚¨áÔ∏è Excel (.xlsx)",
                data=to_xlsx_bytes(scope[col_order] if col_order else scope, sheet="Service_History"),
                file_name=f"Service_History_{sel_asset.replace(' ','_')}.xlsx" if assets else "Service_History.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
        with c2:
            st.download_button(
                "‚¨áÔ∏è Word (.docx)",
                data=to_docx_bytes(scope[col_order] if col_order else scope, title=f"Service History ‚Äî {sel_asset}" if assets else "Service History"),
                file_name=f"Service_History_{sel_asset.replace(' ','_')}.docx" if assets else "Service_History.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            )
        st.stop()

    # ========= Service Procedure Filter =========
    if page == "üõ†Ô∏è Service Procedure Filter":
        st.markdown("### Service Procedure Filter (local workbook)")

        try:
            df_proc, df_ctrl, inv_df, df_xref, name_col = load_procedure_workbook(pxmtime, str(proc_path))
        except Exception as e:
            st.error(f"Error loading procedures workbook: {e}")
            st.stop()

        assets    = sorted(df_ctrl[COL_ASSET].dropna().astype(str).unique().tolist())
        services  = sorted(df_proc[COL_SERVICE].dropna().astype(str).unique().tolist())
        locations = sorted(df_ctrl[COL_LOC_LIST].dropna().astype(str).unique().tolist())

        c1, c2, c3 = st.columns([2,2,3])
        with c1:
            sel_asset   = st.selectbox("Asset", options=assets, index=0 if assets else None)
        with c2:
            sel_service = st.selectbox("Service", options=services, index=0 if services else None)
        with c3:
            sel_loc     = st.selectbox("Location (Controls[Locations] ‚Üí Parts_Master[Location2])", options=locations, index=0 if locations else None)

        if st.button("Run Filter"):
            row = df_ctrl.loc[df_ctrl[COL_ASSET].astype(str) == str(sel_asset)]
            if row.empty:
                st.warning(f"No Serial in Controls for asset: {sel_asset}")
                st.stop()
            serial = str(row.iloc[0][COL_SERIAL])

            mask = (
                df_proc[COL_SERIAL].astype(str).str.upper().eq(serial.upper()) &
                df_proc[COL_SERVICE].astype(str).str.upper().eq(sel_service.upper())
            )
            proc_filt = df_proc.loc[mask].copy()
            if proc_filt.empty:
                st.warning(f"No rows for Serial {serial} / Service {sel_service}.")
                st.stop()

            result = unpivot_to_task_then_parts(proc_filt)
            result["_PN_NORM"] = result["Part Number"].apply(normalize_pn)

            # CAT ‚Üí Donaldson cross reference
            if not df_xref.empty and {XREF_COL_CAT, XREF_COL_DON}.issubset(df_xref.columns):
                xref = df_xref.copy()
                xref["_CAT_NORM"] = xref[XREF_COL_CAT].apply(normalize_pn)
                xref["_DON_RAW"]  = xref[XREF_COL_DON].astype(str)
                result = result.merge(
                    xref[["_CAT_NORM", "_DON_RAW"]].drop_duplicates("_CAT_NORM"),
                    how="left", left_on="_PN_NORM", right_on="_CAT_NORM"
                )
                def map_don(row_):
                    if row_["_PN_NORM"] == "":
                        return ""
                    val = row_.get("_DON_RAW")
                    if pd.isna(val) or str(val).strip() == "":
                        return "No Interchange"
                    return str(val)
                result["Donaldson Interchange"] = result.apply(map_don, axis=1)
                result.drop(columns=["_CAT_NORM", "_DON_RAW"], inplace=True, errors="ignore")
            else:
                result["Donaldson Interchange"] = ""

            # Inventory lookups (Name-first, then Part Numbers; token-based)
            def compute_instk(pn_norm: str) -> str:
                if pn_norm == "":
                    return ""
                hit = excel_like_first_match(inv_df, pn_norm, sel_loc)
                return inv_text_from_row(hit)

            result["InStk"] = result["_PN_NORM"].apply(compute_instk)

            def compute_instock(di: str) -> str:
                if not di or di.strip() == "" or di.strip().upper() == "NO INTERCHANGE":
                    return ""
                di_norm = normalize_pn(di)
                hit = excel_like_first_match(inv_df, di_norm, sel_loc)
                return inv_text_from_row(hit)

            result["In Stock"] = result["Donaldson Interchange"].apply(compute_instock)
            result.drop(columns=["_PN_NORM"], inplace=True, errors="ignore")

            base_cols = [COL_TASKNO, COL_TASK, "Part Number", "Part Type", "Qty",
                         "InStk", "Donaldson Interchange", "In Stock"]
            ordered = [c for c in base_cols if c in result.columns] + [c for c in result.columns if c not in base_cols]

            st.subheader("Filtered Result")
            st.dataframe(result[ordered], use_container_width=True)

            # Downloads
            csv_bytes  = result[ordered].to_csv(index=False).encode("utf-8")
            xlsx_bytes = to_xlsx_bytes(result[ordered], sheet="Filtered")

            try:
                docx_bytes = proc_to_docx_bytes(
                    result[ordered],
                    asset=sel_asset, serial=serial, service=sel_service, location=sel_loc
                )
                have_docx = True
            except Exception:
                have_docx = False
                docx_bytes = b""

            cols = 3 if have_docx else 2
            c = st.columns(cols)
            with c[0]:
                st.download_button("‚¨áÔ∏è CSV", data=csv_bytes,
                                   file_name=f"Filtered_{sel_asset}_{sel_service.replace(' ','_')}.csv",
                                   mime="text/csv")
            with c[1]:
                st.download_button("‚¨áÔ∏è Excel", data=xlsx_bytes,
                                   file_name=f"Filtered_{sel_asset}_{sel_service.replace(' ','_')}.xlsx",
                                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            if have_docx:
                with c[2]:
                    st.download_button("‚¨áÔ∏è Word", data=docx_bytes,
                                       file_name=f"Filtered_{sel_asset}_{sel_service.replace(' ','_')}.docx",
                                       mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
        st.stop()
